{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a0a164-381f-45f4-9a94-910a48684fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4508c5-18fc-4593-9400-35704c2d33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle Depth Anything\n",
    "\n",
    "model = DepthAnythingV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "842dd81f-ed45-4968-ba2e-e03881f1e217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1944, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"dataset/depth/\" + os.listdir(\"dataset/depth/\")[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7566dc99-6b70-4da2-9029-587d5fd81e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1944, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Image.open(\"dataset/images/\" + os.listdir(\"dataset/images/\")[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c5f924-8bbb-4dea-ba3d-10009070d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intégration de LoRA\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, layer, rank=4):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.layer = layer\n",
    "\n",
    "        # Matrices de faible rang (U et V)\n",
    "        self.U = nn.Parameter(torch.randn(layer.in_features, rank) * 0.01)\n",
    "        self.V = nn.Parameter(torch.randn(rank, layer.out_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adaptation avec les matrices de faible rang\n",
    "        return self.layer(x) + torch.matmul(torch.matmul(x, self.U), self.V)\n",
    "\n",
    "# Appliquer LoRA aux couches linéaires du modèle Depth Anything\n",
    "layers_to_modify = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        layers_to_modify.append((name, module))\n",
    "\n",
    "for name, module in layers_to_modify:\n",
    "    lora_layer = LoRA(module)\n",
    "    setattr(model, name, lora_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07197a15-d9c4-4096-88df-6203fc0df098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gel des paramètres non LoRA\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'U' not in name and 'V' not in name:  # Geler les autres paramètres\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a97db4e8-d096-4fe2-8e53-c8066acbe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataset\n",
    "\n",
    "def preprocess_data(image_path, xyz_path, target_size=(224, 224)):\n",
    "    # Charger l'image RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(target_size, Image.Resampling.LANCZOS)  # Redimensionner\n",
    "    image = np.array(image) / 255.0  # Normaliser entre 0 et 1\n",
    "    \n",
    "    # Charger le nuage de points XYZ\n",
    "    xyz = np.load(xyz_path)\n",
    "    \n",
    "    # Redimensionner le nuage de points pour correspondre à la taille cible\n",
    "    xyz_resized = np.zeros((*target_size, 3), dtype=np.float32)\n",
    "    for i in range(3):  # Canaux X, Y, Z\n",
    "        xyz_resized[:, :, i] = np.array(Image.fromarray(xyz[:, :, i]).resize(target_size, Image.Resampling.LANCZOS))\n",
    "    \n",
    "    # Convertir en tenseurs PyTorch\n",
    "    image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()  # (H, W, C) -> (C, H, W)\n",
    "    xyz_tensor = torch.from_numpy(xyz_resized)\n",
    "    \n",
    "    return image_tensor, xyz_tensor\n",
    "\n",
    "# Dataset personnalisé\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_files, point_cloud_dir, point_cloud_files):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = image_files\n",
    "        self.point_cloud_dir = point_cloud_dir\n",
    "        self.point_cloud_files = point_cloud_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        xyz_path = os.path.join(self.point_cloud_dir, self.point_cloud_files[idx])\n",
    "\n",
    "        image, xyz = preprocess_data(image_path, xyz_path)\n",
    "        \n",
    "        return image, torch.tensor(xyz, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "442fa9d2-3c9e-4554-95f2-36ce8612569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "\n",
    "# Paramètres\n",
    "IMAGES_DIR = \"dataset/images\"\n",
    "DEPTH_DIR = \"dataset/depth\"\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Liste des fichiers\n",
    "images_files = sorted(os.listdir(IMAGES_DIR))\n",
    "depth_files = sorted(os.listdir(DEPTH_DIR))\n",
    "\n",
    "# Vérification de correspondance\n",
    "assert len(images_files) == len(depth_files), \"Les dossiers images et depth doivent contenir le même nombre de fichiers.\"\n",
    "assert all(img.split('.')[0][:-6] == depth.split('.')[0][:-9] for img, depth in zip(images_files, depth_files)), \\\n",
    "    \"Les noms des fichiers images et depth doivent correspondre.\"\n",
    "\n",
    "# Division des données\n",
    "train_indices, val_indices = train_test_split(range(len(images_files)), train_size=TRAIN_RATIO, random_state=42)\n",
    "\n",
    "# Création des listes d'entraînement et de validation\n",
    "train_images = [images_files[idx] for idx in train_indices]\n",
    "val_images = [images_files[idx] for idx in val_indices]\n",
    "train_depth = [depth_files[idx] for idx in train_indices]\n",
    "val_depth = [depth_files[idx] for idx in val_indices]\n",
    "\n",
    "# Créations des datasets\n",
    "train_dataset = DepthDataset(IMAGES_DIR, train_images, DEPTH_DIR, train_depth)\n",
    "val_dataset = DepthDataset(IMAGES_DIR, val_images, DEPTH_DIR, val_depth)\n",
    "\n",
    "# Division en mini-lots\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f98cb491-009f-45f7-a2d4-566a44cd09dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estelle\\AppData\\Local\\Temp\\ipykernel_18056\\1587601536.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return image, torch.tensor(xyz, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([224, 224, 3]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "672f8aa6-4d55-431a-8e56-b8e2407fc98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estelle\\AppData\\Local\\Temp\\ipykernel_18056\\1587601536.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return image, torch.tensor(xyz, dtype=torch.float32)\n",
      "C:\\Users\\estelle\\AppData\\Local\\Temp\\ipykernel_18056\\3798512537.py:12: UserWarning: Using a target size (torch.Size([16, 224, 224, 3])) that is different to the input size (torch.Size([16, 224, 224])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred, target)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# Prédire la carte de profondeur\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_maps\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculer la perte\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[62], line 12\u001b[0m, in \u001b[0;36mdepth_loss\u001b[1;34m(pred, target)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdepth_loss\u001b[39m(pred, target):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3794\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Optimiser uniquement les paramètres des couches LoRA\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'U' in name or 'V' in name:  # Ne mettre à jour que les matrices U et V de LoRA\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "optimizer = AdamW(params_to_update, lr=1e-4)\n",
    "\n",
    "# Fonction de perte (par exemple, l'erreur de profondeur)\n",
    "def depth_loss(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "num_epochs = 1\n",
    "# Entraînement\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, depth_maps = batch  # images et leurs cartes de profondeur cibles\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # Prédire la carte de profondeur\n",
    "        loss = depth_loss(outputs, depth_maps)  # Calculer la perte\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "caa6f8d8-6764-490b-bc33-35c24c927d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of outputs: torch.Size([16, 224, 224])\n",
      "Shape of depth_maps: torch.Size([16, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of outputs: {outputs.shape}\")\n",
    "print(f\"Shape of depth_maps: {depth_maps.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6659d61-b4ec-496a-a5e4-a2d7f9b1e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "# Exemple d'évaluation avec un DataLoader de test\n",
    "for batch in test_loader:\n",
    "    images, depth_maps = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    predictions.append(outputs)\n",
    "    targets.append(depth_maps)\n",
    "\n",
    "# Calculer les métriques\n",
    "predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
    "targets = torch.cat(targets, dim=0).cpu().numpy()\n",
    "\n",
    "# Conversion en valeurs discrètes pour le calcul des métriques (si nécessaire)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "targets = (targets > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(targets, predictions)\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835986e0-b372-48de-b73c-39d619551c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des poids du modèle\n",
    "\n",
    "torch.save(model.state_dict(), \"depth_anything_lora_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
