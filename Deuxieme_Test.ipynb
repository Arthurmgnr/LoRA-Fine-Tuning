{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a0a164-381f-45f4-9a94-910a48684fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4508c5-18fc-4593-9400-35704c2d33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle Depth Anything\n",
    "\n",
    "model = DepthAnythingV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c5f924-8bbb-4dea-ba3d-10009070d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intégration de LoRA\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, layer, rank=4):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.layer = layer\n",
    "\n",
    "        # Matrices de faible rang (U et V)\n",
    "        self.U = nn.Parameter(torch.randn(layer.in_features, rank) * 0.01)\n",
    "        self.V = nn.Parameter(torch.randn(rank, layer.out_features) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adaptation avec les matrices de faible rang\n",
    "        return self.layer(x) + torch.matmul(torch.matmul(x, self.U), self.V)\n",
    "\n",
    "# Appliquer LoRA aux couches linéaires du modèle Depth Anything\n",
    "layers_to_modify = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        layers_to_modify.append((name, module))\n",
    "\n",
    "for name, module in layers_to_modify:\n",
    "    lora_layer = LoRA(module)\n",
    "    setattr(model, name, lora_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07197a15-d9c4-4096-88df-6203fc0df098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gel des paramètres non LoRA\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'U' not in name and 'V' not in name:  # Geler les autres paramètres\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97db4e8-d096-4fe2-8e53-c8066acbe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataset\n",
    "\n",
    "# Dataset personnalisé\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, image_dir, image_files, point_cloud_dir, point_cloud_files, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = image_files\n",
    "        self.point_cloud_dir = point_cloud_dir\n",
    "        self.point_cloud_files = point_cloud_files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        xyz_path = os.path.join(self.point_cloud_dir, self.point_cloud_files[idx])\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform(image)\n",
    "        image = np.array(image)\n",
    "        xyz = np.load(xyz_path)\n",
    "        \n",
    "        return image, torch.tensor(xyz, dtype=torch.float32)\n",
    "\n",
    "# Fonction de prétraitement pour redimensionner les images\n",
    "def resize_to_multiple_of_patch(image, patch_size=14):\n",
    "    width, height = image.size\n",
    "    new_width = (width // patch_size) * patch_size\n",
    "    new_height = (height // patch_size) * patch_size\n",
    "    return image.resize((new_width, new_height), Image.BICUBIC)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: resize_to_multiple_of_patch(x, patch_size=14)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442fa9d2-3c9e-4554-95f2-36ce8612569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "\n",
    "# Paramètres\n",
    "IMAGES_DIR = \"dataset/images\"\n",
    "DEPTH_DIR = \"dataset/depth\"\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# Liste des fichiers\n",
    "images_files = sorted(os.listdir(IMAGES_DIR))\n",
    "depth_files = sorted(os.listdir(DEPTH_DIR))\n",
    "\n",
    "# Vérification de correspondance\n",
    "assert len(images_files) == len(depth_files), \"Les dossiers images et depth doivent contenir le même nombre de fichiers.\"\n",
    "assert all(img.split('.')[0][:-6] == depth.split('.')[0][:-9] for img, depth in zip(images_files, depth_files)), \\\n",
    "    \"Les noms des fichiers images et depth doivent correspondre.\"\n",
    "\n",
    "# Division des données\n",
    "train_indices, val_indices = train_test_split(range(len(images_files)), train_size=TRAIN_RATIO, random_state=42)\n",
    "\n",
    "# Création des listes d'entraînement et de validation\n",
    "train_images = [images_files[idx] for idx in train_indices]\n",
    "val_images = [images_files[idx] for idx in val_indices]\n",
    "train_depth = [depth_files[idx] for idx in train_indices]\n",
    "val_depth = [depth_files[idx] for idx in val_indices]\n",
    "\n",
    "# Créations des datasets\n",
    "train_dataset = DepthDataset(IMAGES_DIR, train_images, DEPTH_DIR, train_depth, transform)\n",
    "val_dataset = DepthDataset(IMAGES_DIR, val_images, DEPTH_DIR, val_depth, transform)\n",
    "\n",
    "# Division en mini-lots\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f8aa6-4d55-431a-8e56-b8e2407fc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser uniquement les paramètres des couches LoRA\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'U' in name or 'V' in name:  # Ne mettre à jour que les matrices U et V de LoRA\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "optimizer = AdamW(params_to_update, lr=1e-4)\n",
    "\n",
    "# Fonction de perte (par exemple, l'erreur de profondeur)\n",
    "def depth_loss(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "num_epochs = 1\n",
    "# Entraînement\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, depth_maps = batch  # images et leurs cartes de profondeur cibles\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # Prédire la carte de profondeur\n",
    "        loss = depth_loss(outputs, depth_maps)  # Calculer la perte\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6659d61-b4ec-496a-a5e4-a2d7f9b1e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "# Exemple d'évaluation avec un DataLoader de test\n",
    "for batch in test_loader:\n",
    "    images, depth_maps = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    predictions.append(outputs)\n",
    "    targets.append(depth_maps)\n",
    "\n",
    "# Calculer les métriques\n",
    "predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
    "targets = torch.cat(targets, dim=0).cpu().numpy()\n",
    "\n",
    "# Conversion en valeurs discrètes pour le calcul des métriques (si nécessaire)\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "targets = (targets > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(targets, predictions)\n",
    "precision = precision_score(targets, predictions)\n",
    "recall = recall_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835986e0-b372-48de-b73c-39d619551c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des poids du modèle\n",
    "\n",
    "torch.save(model.state_dict(), \"depth_anything_lora_finetuned.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
