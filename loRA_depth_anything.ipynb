{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5acd52-66ac-4c8b-9eaf-c85fc22876be",
   "metadata": {},
   "source": [
    "# Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10519c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torch\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import DefaultDataCollator\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cda279-3de8-4096-a35a-fa5359eecb63",
   "metadata": {},
   "source": [
    "# Chargement du modèle DepthAnything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "encoder = 'vits' # or 'vits', 'vitb', 'vitg'\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[encoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c53a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8c/9q4rw3jn16712tpdyjqy22l40000gn/T/ipykernel_50729/3547727159.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'depth_anything_v2_{encoder}.pth', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'depth_anything_v2_{encoder}.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113697e4-c561-4a64-b550-1120552ea63f",
   "metadata": {},
   "source": [
    "# Création d'une classe pour utiliser LoRA dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda479b1-60d9-4f42-924f-a38c4129adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n",
    "    def __init__(self, module: nn.Linear, rank: int):\n",
    "        super().__init__()\n",
    "        self.module = module  # pre-trained (frozen) linear layer\n",
    "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n",
    "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n",
    "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n",
    "        adapter_output = input @ self.adapter_A @ self.adapter_B\n",
    "        module_output = self.module(input)\n",
    "        return module_output + adapter_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892810e6-8211-47e8-97a0-1637557e6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "# Assurez-vous que le modèle possède un attribut 'model' (si c'est un modèle transformer, par exemple)\n",
    "# Si le modèle a des blocs d'attention dans un autre sous-modèle, ajustez cette ligne pour correspondre à la structure de votre modèle\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # Vérifier si le module est une couche d'attention, ici on cherche les projections QKV et proj\n",
    "    if isinstance(module, nn.MultiheadAttention):  # Vérifie que le module est une couche d'attention multi-têtes\n",
    "        print(f\"Applying LoRA to {name}\")\n",
    "\n",
    "        # Appliquer LoRA sur les projections Q, K, V\n",
    "        if hasattr(module, 'in_proj_weight'):\n",
    "            # Le poids QKV pour la couche d'attention multi-têtes\n",
    "            qkv_weight = module.in_proj_weight\n",
    "            module.in_proj_weight = LoRALayer(module.in_proj_weight, rank=lora_rank).to(qkv_weight.device)\n",
    "\n",
    "        # Appliquer LoRA sur la couche de projection de sortie\n",
    "        if hasattr(module, 'out_proj'):\n",
    "            out_proj_weight = module.out_proj.weight\n",
    "            module.out_proj = LoRALayer(module.out_proj, rank=lora_rank).to(out_proj_weight.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e1d7e-c117-43de-95d0-c8f4a9b9eec5",
   "metadata": {},
   "source": [
    "# Classe pour parcourir le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d9fbdf-896e-45b1-8ac2-695a5bcf0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, image_dir, depth_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.depth_dir = depth_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.depth_files = sorted(os.listdir(depth_dir)) # sort pour assurer le traitement d'apres\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        depth_path = os.path.join(self.depth_dir, self.depth_files[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        #depth = Image.open(depth_path).convert(\"L\")  # Assuming depth maps are grayscale\n",
    "        depth = np.load(depth_path)\n",
    "\n",
    "\n",
    "        return {\"image\": image, \"depth\": depth}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b5f3f-af33-4c82-b7e7-b30be07ad9eb",
   "metadata": {},
   "source": [
    "# Classe pour faire je sais pas quoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f7e8b8-402c-4e25-8b08-4f55d15c4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataCollator(DefaultDataCollator):\n",
    "    def __call__(self, features):\n",
    "        images = [feature[\"images\"] for feature in features]\n",
    "        depths = [feature[\"depth\"] for feature in features]\n",
    "        return {\"images\": torch.stack(images), \"depths\": torch.stack(depths)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eddf92-262d-4767-b9a7-0087c36da2f4",
   "metadata": {},
   "source": [
    "# Fonction permettant de calculer des métriques pour analyser le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c028f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(dim=1).detach().cpu().numpy()  # Prédictions des classes\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2de9e0-4942-4bd1-b3d0-470ccd129943",
   "metadata": {},
   "source": [
    "# Code principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2dfb24e-4c57-4c43-b1a4-ff6f8adc6c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     13\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(Trainer(\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(accelerator\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     22\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     23\u001b[0m ))\n\u001b[1;32m     25\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/LoRA/lib/python3.10/site-packages/accelerate/accelerator.py:495\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    493\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m ):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "img_dir = \"C:/Users/yoyoc/Desktop/DATASET_DEVOIR/DATASET_DEVOIR/images\"\n",
    "dpt_dir = \"C:/Users/yoyoc/Desktop/DATASET_DEVOIR/DATASET_DEVOIR/depth\"\n",
    "\n",
    "img_dir = \"dataset/images\"\n",
    "dpt_dir = \"dataset/depth\"\n",
    "\n",
    "train_dataset = DepthDataset(image_dir=img_dir, depth_dir=dpt_dir)\n",
    "\n",
    "data_collator = DepthDataCollator()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    output_dir='outputs',\n",
    "    report_to=None\n",
    ")\n",
    "accelerator = Accelerator()\n",
    "trainer = accelerator.prepare(Trainer(\n",
    "    model=model.to(accelerator.device),\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "))\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
