{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e61bc7-7606-4e1c-9438-4f567411b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "# Importation des librairies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc5c025-ba44-43ea-a2d5-f5ed6adc19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement des données\n",
    "\n",
    "# Transformation des images RGB et des nuages de points\n",
    "def transform_image_and_point_cloud(image, point_cloud, target_height=1200, target_width=1944):\n",
    "    \"\"\"\n",
    "    Redimensionne l'image et le nuage de points pour qu'ils aient des dimensions multiples de 14\n",
    "    tout en maintenant le ratio d'aspect.\n",
    "    \"\"\"\n",
    "    # Redimensionner l'image RGB\n",
    "    image_resized = resize_to_multiple_of_14(image, target_height, target_width)\n",
    "    \n",
    "    # Redimensionner le nuage de points de manière similaire\n",
    "    point_cloud_resized = cv2.resize(point_cloud, (image_resized.shape[1], image_resized.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return image_resized, point_cloud_resized\n",
    "\n",
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, point_cloud_dir, transform=None):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.point_cloud_dir = point_cloud_dir\n",
    "        self.rgb_files = sorted(os.listdir(rgb_dir))  # Liste des fichiers PNG\n",
    "        self.point_cloud_files = sorted(os.listdir(point_cloud_dir))  # Liste des fichiers NPY\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Chargement de l'image RGB\n",
    "        rgb_image = cv2.imread(os.path.join(self.rgb_dir, self.rgb_files[idx]))\n",
    "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)  # Conversion BGR -> RGB\n",
    "\n",
    "        # Chargement du nuage de points\n",
    "        point_cloud = np.load(os.path.join(self.point_cloud_dir, self.point_cloud_files[idx]))\n",
    "\n",
    "        # Redimensionner les images et les nuages de points\n",
    "        rgb_image, point_cloud = resize_image_and_point_cloud(rgb_image, point_cloud)\n",
    "\n",
    "        # Optionnel : Transformer les images et nuages de points (normalisation ou autres)\n",
    "        if self.transform:\n",
    "            rgb_image = self.transform(rgb_image)\n",
    "            point_cloud = self.transform(point_cloud)\n",
    "\n",
    "        return torch.tensor(rgb_image, dtype=torch.float32), torch.tensor(point_cloud, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6efd4a7-b886-4655-a86a-89f40b473169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Définir LoRA\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, rank, input_dim, output_dim):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.A = nn.Parameter(torch.randn(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.matmul(torch.matmul(x, self.A), self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca33aaf-ea35-4d92-a788-68295732bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modèle Depth Anything + Intégration de LoRA\n",
    "\n",
    "class DepthAnythingWithLoRA(nn.Module):\n",
    "    def __init__(self, base_model, rank=8):\n",
    "        super(DepthAnythingWithLoRA, self).__init__()\n",
    "        self.base_model = base_model  # Modèle pré-entraîné Depth Anything (tu devras charger ce modèle)\n",
    "        self.lora_layers = nn.ModuleList([\n",
    "            LoRA(rank=rank, input_dim=512, output_dim=512),  # Exemple de taille\n",
    "            # Ajoute d'autres couches LoRA si nécessaire pour les couches du modèle\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)  # Passage à travers le modèle Depth Anything\n",
    "        for layer in self.lora_layers:\n",
    "            x = layer(x)  # Application de LoRA\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d89eaac-1ef5-4904-b9c6-84e287b3a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Entraînement et fine-tuning\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images_rgb, point_clouds in dataloader:\n",
    "            images_rgb = images_rgb.permute(0, 3, 1, 2)  # Convertir en (batch, 3, H, W)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Prédiction de la profondeur\n",
    "            predicted_depth = model(images_rgb)\n",
    "\n",
    "            # Calcul de la perte\n",
    "            loss = criterion(predicted_depth, point_clouds)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d9b198e-e9db-4976-a049-4fad567d233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Évaluation du modèle\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predicted = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for images_rgb, point_clouds in dataloader:\n",
    "            images_rgb = images_rgb.permute(0, 3, 1, 2)\n",
    "            predicted_depth = model(images_rgb)\n",
    "\n",
    "            all_predicted.append(predicted_depth.cpu().numpy())\n",
    "            all_true.append(point_clouds.cpu().numpy())\n",
    "\n",
    "    all_predicted = np.concatenate(all_predicted, axis=0)\n",
    "    all_true = np.concatenate(all_true, axis=0)\n",
    "\n",
    "    # Calcul de l'erreur MAE et RMSE\n",
    "    mae = mean_absolute_error(all_true, all_predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(all_true, all_predicted))\n",
    "    print(f\"MAE: {mae}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3da603ee-56b7-4d55-b37b-9d36256edb80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Input image height 1196 is not a multiple of patch height 14",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Perte pour prédiction de la profondeur\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 5. Fine-tuning\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 6. Sauvegarde du modèle fine-tuné\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), 'fine_tuned_depth_anything.pth')\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 7. Évaluation du modèle\u001b[39;00m\n\u001b[0;32m     34\u001b[0m evaluate(model, dataloader)\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prédiction de la profondeur\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m predicted_depth \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calcul de la perte\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_depth, point_clouds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mDepthAnythingWithLoRA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Passage à travers le modèle Depth Anything\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_layers:\n\u001b[0;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)  \u001b[38;5;66;03m# Application de LoRA\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Arthur\\LoRA-Fine-Tuning\\depth_anything_v2\\dpt.py:179\u001b[0m, in \u001b[0;36mDepthAnythingV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    177\u001b[0m     patch_h, patch_w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m--> 179\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_layer_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_class_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_head(features, patch_h, patch_w)\n\u001b[0;32m    182\u001b[0m     depth \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(depth)\n",
      "File \u001b[1;32m~\\Documents\\Arthur\\LoRA-Fine-Tuning\\depth_anything_v2\\dinov2.py:308\u001b[0m, in \u001b[0;36mDinoVisionTransformer.get_intermediate_layers\u001b[1;34m(self, x, n, reshape, return_class_token, norm)\u001b[0m\n\u001b[0;32m    306\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_intermediate_layers_chunked(x, n)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_intermediate_layers_not_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32m~\\Documents\\Arthur\\LoRA-Fine-Tuning\\depth_anything_v2\\dinov2.py:272\u001b[0m, in \u001b[0;36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001b[1;34m(self, x, n)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_intermediate_layers_not_chunked\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 272\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tokens_with_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# If n is an int, take the n last blocks. If it's a list, take them\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     output, total_block_len \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n",
      "File \u001b[1;32m~\\Documents\\Arthur\\LoRA-Fine-Tuning\\depth_anything_v2\\dinov2.py:214\u001b[0m, in \u001b[0;36mDinoVisionTransformer.prepare_tokens_with_masks\u001b[1;34m(self, x, masks)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_tokens_with_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    213\u001b[0m     B, nc, w, h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(masks\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\LoRA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Arthur\\LoRA-Fine-Tuning\\depth_anything_v2\\dinov2_layers\\patch_embed.py:73\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m _, _, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     71\u001b[0m patch_H, patch_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m%\u001b[39m patch_H \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image height \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a multiple of patch height \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m W \u001b[38;5;241m%\u001b[39m patch_W \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image width \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a multiple of patch width: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatch_W\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)  \u001b[38;5;66;03m# B C H W\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Input image height 1196 is not a multiple of patch height 14"
     ]
    }
   ],
   "source": [
    "# Code principal\n",
    "\n",
    "# Paramètres\n",
    "rgb_dir = 'dataset/images'\n",
    "point_cloud_dir = 'dataset/depth'\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "rank = 8  # Taille du rang pour LoRA\n",
    "\n",
    "# 1. Préparation des données\n",
    "dataset = DepthDataset(rgb_dir, point_cloud_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 2. Charger le modèle pré-entraîné Depth Anything\n",
    "# Remplace cette partie par le chargement réel du modèle Depth Anything\n",
    "#base_model = torch.hub.load('depth-anything-v2.github.io', 'depth_anything_pretrained', pretrained=True)\n",
    "base_model = DepthAnythingV2()\n",
    "\n",
    "# 3. Ajouter LoRA au modèle\n",
    "model = DepthAnythingWithLoRA(base_model, rank=rank)\n",
    "\n",
    "# 4. Optimiseur et critère\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()  # Perte pour prédiction de la profondeur\n",
    "\n",
    "# 5. Fine-tuning\n",
    "train(model, dataloader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# 6. Sauvegarde du modèle fine-tuné\n",
    "#torch.save(model.state_dict(), 'fine_tuned_depth_anything.pth')\n",
    "\n",
    "# 7. Évaluation du modèle\n",
    "evaluate(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
